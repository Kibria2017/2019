---
title: Unsupervised Analyses
date: Nov 24, 2019
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
fig_width: 5
fig_height: 5
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center", 
                      out.width = '70%', 
                      cache = TRUE)
```

First, we will install a few R package 
```{r,eval=FALSE}
install.packages(rafalib)
```

Then, we load a few R packages
```{r, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(rafalib)
```

# Motivation

In this lecture, we will be analyzing some
genomics data measured through high-throughput 
technologies, which measure thousands of features
at a time. Examples of features are image pixel 
intensities, genes, single base locations of the 
genome, or genomic regions. Each specific 
measurement product is defined by a specific 
set of features. For example, a specific gene 
expression microarray product is defined by the 
set of genes that it measures.

The lecture material came from these resources: 

* [http://genomicsclass.github.io/book/pages/clustering_and_heatmaps.html](http://genomicsclass.github.io/book/pages/clustering_and_heatmaps.html)
* [http://genomicsclass.github.io/book/pages/eda_with_pca.html](http://genomicsclass.github.io/book/pages/eda_with_pca.html)

A specific study will typically use one product to 
make measurements on several experimental units, 
such as individuals. The most common experimental 
unit will be the individual, but they can also be 
defined by other entities, for example different 
parts of a tumor. 

Here, we show an example for which we measure RNA 
expression for 8,793 genes from blood taken from 
208 individuals. In this case the data was originally 
collected to compare gene expression across ethnic
groups. The study is described in [this paper](http://www.ncbi.nlm.nih.gov/pubmed/17206142),
which claimed that roughly 50% of genes where 
differentially expressed when comparing blood from 
two ethnic groups. 

In the lecture about dimensionality reduction, we 
talked about having a set of $n$ observations
$X_{1},...,X_{n}$ each with $p$ features. The goal 
there was to understand the relationships between 
them or patterns in them. After identifying the most 
informative features, we might be interested in 
grouping the observations or samples in some way 
(e.g. clustering approaches). Both of these are
a form of _unsupervised_ analyses. 

Some types of techniques for unsupervised analyses 
include: 

* Clustering
* Principal components analysis/SVD
* Factor analysis
* Kernel density estimation
* Multidimensional scaling (MDS) 
* ICA 
* ... and many more!

In contrast, we have already discussed
the _supervised_ setting where we have some covariates 
$X$ and some outcome $Y$ and we are interested in 
solving something like $\arg\min_{f} E[(Y−f(X))^2]$.

In this lecture, we will use exploratory data analysis 
(EDA) and apply unsupervised methods to ask the questions:

**How big is the variation between different ethnic groups? Do samples from the different ethnic groups cluster by ethnicity or by something else?**

## Data 

The data we will use comes from 
[this paper](https://www.ncbi.nlm.nih.gov/pubmed/17206142) 
which looked at differences in gene expression data 
across three ethnic populations. 

A [gene expression](https://en.wikipedia.org/wiki/Gene_expression)
data set obtained from a 
[microarray](https://en.wikipedia.org/wiki/DNA_microarray) 
experiment. [Read more about the specific experiment here](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5859). 
There are two data sets we will use (both are available on GitHub
in the course repo [`/data`](https://github.com/jhu-advdatasci/2019/tree/master/data)): 

1. The gene expression intensities (called `exprs_GSE5859.csv`) where the rows represent the features on the microarray (e.g. genes) and the columns represent the different microarray samples.
2. An annotation table (called `sampleinfo_GSE5859.csv`) that contains the information about each of the samples (columns in the gene expression data set) such as the sex, the age, the treatment status, the date the samples were processed. Each row represents one sample.
 
The data are available on the GitHub course repo `/data`, 
so once we pull the changes locally, we can read in the 
data into R. 

```{r, warning=FALSE, message=FALSE}
exprs <- read_csv(here("data", "GSE5859_exprs.csv"))
sampleinfo <- read_csv(here("data", "GSE5859_sampleinfo.csv"))
```

Let's take a peek at the `exprs` data


```{r}
dim(exprs) 
exprs[1:5, 1:5]
```

We see there is one column called `X1` and then others that all 
start with `GSM`. The first column describes the $p=8793$ 
features and the $N=208$ observations (or microarray samples). 

Let's get rid of that first column and convert the data frame 
into a matrix.

```{r}
exprs <- as.matrix(exprs[,-1])
```

We could plot what the gene expression looks like for all the 
genes in the first four observations (or microarray samples)

```{r}
par(mfrow=c(2,2))
hist(exprs[,1])
hist(exprs[,2])
hist(exprs[,3])
hist(exprs[,4])
```

We see it looks unimodal with a right-skewed tail. 

Boxplots are often another good way to plot a reasonable
number of samples. 

```{r}
boxplot(exprs,range=0,names=1:ncol(exprs))
```

Note that the number of samples is a bit too large here, 
making it hard to see the boxes. One can instead simply 
show the boxplot summaries without the boxes:

```{r}
qs <- t(apply(exprs,2,quantile,prob=c(0.05,0.25,0.5,0.75,0.95)))
matplot(qs,type="l",lty=1)
```

We refer to this figure as a _kaboxplot_ because 
[Karl Broman](https://kbroman.org) was the first we saw 
use it as an alternative to boxplots.

We can also explore the `sampleinfo` table to see what's in it. 

```{r}
head(sampleinfo)
```

We see there is information about `ethnicity`, `date`, 
the `filename` (or microarray sample), and the `sex` 
of the individual. 

How many ethnicities are there in this dataset? 

```{r}
table(sampleinfo$ethnicity)
```

Looks like there are three: 

- ASN = Japanese in Tokyo, Japan
- CEU = Northern Europeans from Utah
- HAN = Chinese in Beijing, China

Now, if we want to use this information (e.g. `ethnicity`), 
to see how the gene expression changes across ethnicity, then 
we will need to make sure the columns in the `exprs` matrix
match the order of the rows of the `sampleinfo` table. 

For example, we can look at `exprs`

```{r}
exprs[1:5,1:5]
```

We can compare the column names in the `exprs` matrix to 
the `sampleinfo$filename` column. If they are different, 
then we will need to re-order them. 

```{r}
sampleinfo$filename == colnames(exprs)
```

Two useful functions are `any()` and `all()` which 
take as input a set of logical vectors and test if 
any or all the logical vectors are true. 

```{r}
all(sampleinfo$filename == colnames(exprs))
```

Nope! They are not the same order. Let's fix that. 

Here we re-order the columns of the `exprs` dataset 
using the `match()` function to make sure the order
of the columns in the `exprs` matrix match the 
order of the rows of the `sampleinfo` table. 

```{r}
exprs <- exprs[,match(sampleinfo$filename,colnames(exprs))]

# sanity check
all(sampleinfo$filename == colnames(exprs))
```

# Clustering

If we want to cluster samples into groups, 
some questions you might immediately ask are: 

* How do we define close?
* How do we group things?
* How do we visualize the grouping?
* How do we interpret the grouping?

## Distance

First, let's talk about how we can define "distance". 
Consider the location of Baltimore and Washington DC 
defined with (X,Y) coordiante points on a Cartesian 
plane. 

```{r, echo=FALSE}
knitr::include_graphics("https://github.com/jtleek/advdatasci/raw/master/imgs/unsupervised/distance.png")
```

[[image source](http://rafalab.jhsph.edu/688/lec/lecture5-clustering.pdf)]

### Euclidean distance

The euclidean distance between 
Baltimore and DC is:

$$\sqrt{ (X_1-X_2)^2 + (Y_1-Y_2)^2} = (|X_1-X_2|^{2} + |Y_1-Y_2|^{2})^{1/2}$$
or 

```{r, echo=FALSE}
knitr::include_graphics("https://github.com/jtleek/advdatasci/raw/master/imgs/unsupervised/euclid.png")
```

We have previously learned how to use the 
`dist()` function to calculate the Euclidean distance. 

Let's pretend that we don’t know these are different 
ethnicities and are interested in clustering. The 
first step is to compute the distance between each sample:

```{r}
d = dist(t(exprs), method = "euclidean")
str(as.matrix(d)) # should be 208 x 208
```

We can look at the pairwise distances using the `image()` function. 

```{r}
image(as.matrix(d))
```

The image suggests there is at least one cluster of
individuals, but unclear about much else. 

### Manhattan distance

In constrast to a Euclidean distance, a 
Manhattan (or taxicab) distance is the 
sum of the absolute differences of the 
Cartesian coordiantes. 

$$|X_1-X_2| + |Y_1-Y_2|$$
The term "taxicab" comes from the distance a car would
drive in a city laid out in square blocks 
(if there are no one-way streets). The taxicab metric 
is also known as rectilinear distance or $L_1$ distance. 

In the picture below, the the red, yellow, and blue paths
(taxicab geometry) all have the same shortest path length
of 12. In Euclidean geometry, the green line has length 
$6\sqrt{2} \approx 8.49$, and is the unique shortest path.

```{r, echo=FALSE}
knitr::include_graphics("https://github.com/jtleek/advdatasci/raw/master/imgs/unsupervised/manhattan.png")
```

[[image source](http://en.wikipedia.org/wiki/Taxicab_geometry)]

There are lots of other distance metrics too such as

- **Minkowski distance** = generalized $L_p$-norm of the difference

$$ d_p(x,y) = || x - y ||_{p} = ( \sum_{i=1}^n |x_i - y_i|^{p})^{1/p} $$

- **Chebyshev distance** =  is the $L_{\infty}$-norm of the difference, a special case of the Minkowski distance where $p$ goes to infinity. It is also known as Chessboard distance.

$$ d(x,y) = || x - y ||_{\infty} = \underset{p\rightarrow \infty}{\lim} ( \sum_{i=1}^n |x_i - y_i|^{p})^{1/p} = \underset{i}{\max} |x_i - y_i | $$

- **Canberra distance** = a weighted version of the Manhattan distance, introduced and refined 1967 by Lance, Williams and Adkins. It is often used for data scattered around an origin, as it is biased for measures around the origin and very sensitive for values close to zero.

$$ d(x,y) =  \sum_{i=1}^n \frac{|x_i - y_i|}{|x_i| + |y_i|} $$

- **Pearson's distance** = a correlation distance based on Pearson's product-momentum correlation coefficient of the two sample vectors. Since the correlation coefficient falls between [-1, 1], the Pearson distance lies in [0, 2] and measures the linear relationship between the two vectors.

$$ d(x,y) = 1 - \textbf{Corr}(x,y) $$

- **Cosine distance** = contains the dot product scaled by the product of the Euclidean distances from the origin. It represents the angular distance of two vectors while ignoring their scale.


- **Hamming distance** = metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different. It is a fundamental distance measure in information theory but less relevant in non-integer numerical problems.

## Hierarchical clustering 

Now that we have computed the distance between each 
of the samples, how can we cluster them into groups? 

Once approach is called _Hierarchical clustering_. 
This is an agglomerative ("bottom up" aka each 
observation starts in its own cluster, and pairs 
of clusters are merged as one moves up the hierarchy) 
approach: 

* Initially: every sample is its own group
* Then, find closest two things
* Put them together
* Find next closest, until there is only one group

It requires

* A defined distance
* A merging approach

It produces: 

* A tree showing how close things are to each other

For this, we will use the `hclust()` function.

```{r}
hc <- hclust(d)
```

This function returns an `hclust` object that 
describes the groupings that were created using 
the algorithm described above. 

```{r}
hc
```

The `plot()` method represents these
relationships with a tree or dendrogram:

```{r}
plot(hc,labels=sampleinfo$ethnicity,cex=0.5)
```

It's hard to see the "clusters". One idea would be to 
color each sample by the ethnic group. For this, we will 
use the `mypclust()` function in the 
[`rafalib`](https://cran.r-project.org/package=rafalib)
R package. 

```{r}
library(rafalib)
myplclust(hc, labels=sampleinfo$ethnicity, 
          lab.col=as.numeric(as.factor(sampleinfo$ethnicity)), cex=0.5)
```

We see that there seem to be some clusters, but 
it's not very clear. There are CEU samples clustering 
together with the ASN samples, etc. 

Also, note that hierarchical clustering does not 
define specific clusters, but rather defines the 
dendrogram above. 

From the dendrogram we can decipher 
the distance between any two groups by looking at 
the height at which the two groups split into two. 

To define clusters, we need to "cut the tree"" at 
some distance and group all samples that are within 
that distance into groups below. To visualize this, 
we draw a horizontal line at the height we wish
to cut and this defines that line. 

We use 73 as an example:

```{r}
myplclust(hc, labels=sampleinfo$ethnicity, 
          lab.col=as.fumeric(sampleinfo$ethnicity), cex=0.5)
abline(h=73)
```

If we use the line above to cut the tree into 
clusters, we can examine how the clusters overlap 
with the actual ethnic groups:

```{r}
hclusters <- cutree(hc, h=73)
table(true=sampleinfo$ethnicity, 
      cluster=hclusters)
```

We can also ask cutree to give us back a given 
number of clusters. The function then automatically 
finds the height that results in the requested
number of clusters:

```{r}
hclusters <- cutree(hc, k=3)
table(true=sampleinfo$ethnicity, 
      cluster=hclusters)
```

In both cases, we do not really see the samples
clustering by ethnicity. For example, the CEU samples 
are spread across all three clusters. We will continue 
to explore this result to see what's going on. 

Before we leave this section, it's important to
note that selecting the number of clusters 
is generally a challenging step in practice 
and an active area of research.

## $k$-means clustering

Another approach to cluster is with the `kmeans()` 
function to perform $k$-means clustering. 

This is a partioning approach. Given a set of 
$N$ observations ($\mathbf{x}$) and a fixed 
number of clusters ($k$), partion the data 
into $k$ clusters. 

More formally, $k$-means clustering aims at 
minimizing the within-cluster sum of squares: 

$$ \underset{S}{\arg \max} \sum_{i=1}^k \sum_{x\in S_i} ||x-\mu_i||^2 $$

In practice, we use an iterative algorithm based on two steps: 

1. **Assignment**: given a set of centroids, assign each observation to the closet centroid. 
2. **Update**: compute new centroids for each cluster. 

Requires

* A defined distance metric
* A number of clusters
* An initial guess as to cluster centroids (often random)

Produces

* Final estimate of cluster centroids
* An assignment of each point to clusters

Let's run $k$-means on the samples in the 
space of the first two genes:

```{r}
set.seed(1000)
km <- kmeans(t(exprs[1:2,]), centers=3)
names(km)
```

If we plot the observations by ethnicity:

```{r}
new.dat <- data.frame(t(exprs[1:2,]), 
                      sampleinfo, 
                      "cluster"=factor(km$cluster)) 
new.dat %>% 
  ggplot(aes(x=X1, y=X2, color=ethnicity)) + 
  geom_point() + xlab("Gene 1 expression") + 
  ylab("Gene 2 expression")
```

Since we don't really see a great clusters, we might 
suspect k-means will have a hard time identifying 
the correct clusters. 

However, it's important to note that $k$-means will always
find clusters

```{r}
new.dat %>%
  ggplot(aes(x=X1, y=X2, color = cluster)) + 
  geom_point()
```

As suspected, we see $k$-means did not perform well: 

```{r}
table(true=sampleinfo$ethnicity,
      cluster=km$cluster)
```

This is very likely due to the fact the the 
first two genes are not informative regarding 
ethnicity. We can see this in the first plot 
above. If we instead perform $k$-means clustering 
using all of the genes:

```{r}
km <- kmeans(t(exprs), centers=3)
```

And then plot the samples along the first PCs
(note that the columns -- or genes here -- are 
are centered by default)

```{r}
pc <- prcomp(t(exprs))
str(pc)
```

Here is a plot of the first two PCs. 
We can see that the first two PCs will in fact
be quite informative.

```{r}
new.dat <- data.frame(pc$x[,1:2], sampleinfo, 
                      "cluster"=factor(km$cluster)) 
new.dat %>% 
  ggplot(aes(x=PC1, y=PC2, color = cluster, 
             shape = ethnicity)) + geom_point()
```

Note that it does in fact separate individuals by ethnicity. 
However, this visualization does illustrate a concerning 
characteristic: the CEU points seem to have sub-clusters. 

What are these?

It turns out the date in which the microarray samples were
processed also explain the clusters:

```{r}
library(lubridate)
sampleinfo$exprs_date <- ymd(sampleinfo$date)
sampleinfo$month <- month(sampleinfo$exprs_date )
sampleinfo$year <- year(sampleinfo$exprs_date )

new.dat <- data.frame(pc$x[,1:2], sampleinfo, 
                      "cluster"=factor(km$cluster)) 
new.dat %>% 
  ggplot(aes(x=PC1, y=PC2, color = factor(year), 
             shape = ethnicity)) + geom_point()
```

Let's explore this a bit more. 

What's the earliest date in our dataset? 
```{r}
min(sampleinfo$exprs_date)
```

OK, let's convert the dates into days since Oct 31, 2002
```{r}
sampleinfo$days <- as.numeric(sampleinfo$exprs_date - min(sampleinfo$exprs_date) )
new.dat <- data.frame(pc$x[,1:2], sampleinfo, "cluster"=factor(km$cluster)) 
head(new.dat)
```

Now we will use EDA to explore how date has a large effect 
on the data. 

Consider only the CEU ethnicity and compute the projection
to the first principal component like this

```{r}
ind <- which(sampleinfo$ethnicity=="CEU")
pc <- prcomp(t(exprs[,ind]))
str(pc)

new.dat <- data.frame(pc$x[,1:2], sampleinfo[ind,], 
                      "cluster"=factor(km$cluster[ind])) 
```

If we create a histogram of the first PC
```{r}
hist(pc$x[,1], nc=25)
```

We see two modes. Now let's plot the 
first PC against the date the samples 
were processed: 

```{r}
new.dat %>% 
  ggplot(aes(x=days, y=PC1)) + geom_point() + 
  ggtitle("Relationship between the PC1 and the date the samples were processed") + 
  xlab("Days since first sample was processed") + 
  ylab("Principal Component 1") 
```

Around what time do you notice a difference in
the way the samples were processed?

```{r}
new.dat %>% 
  ggplot(aes(x=days, y=PC1, color=factor(cluster))) + geom_point() + 
  ggtitle("Relationship between the PC1 and the date the samples were processed") + 
  xlab("Days since first sample was processed") + 
  ylab("Principal Component 1") + 
  geom_vline(aes(xintercept=100))
```

We see a change around day 100, which also 
corresponds to the $k$-means cluster predictions. 

## Model based clustering

One disadvantage of hierarchical clustering algorithms,
and k-means algorithms is that they are largely heuristic
and not based on formal models (meaning sample observations 
arise from a distribution that is a mixture of two or more
components).

### Example: A pathological case

```{r}
clust1 = data.frame(x=rnorm(100),y=rnorm(100))

a = runif(100,0,2*pi)
clust2 = data.frame(x=8*cos(a) + rnorm(100),y=8*sin(a) + rnorm(100))

plot(clust2,col='blue',pch=19); points(clust1,col='green',pch=19)
```

If we apply `kmeans()`, we see

```{r}
dat = rbind(clust1,clust2)
kk = kmeans(dat,centers=2)
plot(dat,col=(kk$clust+2),pch=19)
```

## Summary of clustering approaches

* Algomerative (h-clustering) versus divisive (k-means)
* Distance matters!
* Merging matters!
* Number of clusters is rarely estimated in advance
* H-clustering: Deterministic - but you don’t get a fixed number of clusters
* K-means: Stochastic - fix the number of clusters in advance
* Model based: Can select the number of clusters, may be stochastic, careful about assumptions!

### What are some common evaluation methods used in cluster?

1. Sum of Squared Errors (SSE). If a cluster is very "compact" and well separated from others, it will have a small SSE.
2. If you have an external measure (i.e. gold standard), you can also try these: 
  * Rand Index. Compares the two clusters and tries to find the ratio of matching and unmatched observations among two clustering structures. Its value lies between 0 and 1. 
  * Precision Recall Measure. Derived from the confusion matrix. Recall is also known as "Sensitivity" [True Positive/ (True Positive + False Negative)]. For clustering, we use this measure from an information retrieval point of view. Here, precision is a measure of correctly retrieved items. Recall is measure of matching items from all the correctly retrieved items.


